{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8337592,"sourceType":"datasetVersion","datasetId":4951606},{"sourceId":8429393,"sourceType":"datasetVersion","datasetId":5019688}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-27T12:47:58.060101Z","iopub.execute_input":"2024-05-27T12:47:58.060611Z","iopub.status.idle":"2024-05-27T12:47:58.080265Z","shell.execute_reply.started":"2024-05-27T12:47:58.060575Z","shell.execute_reply":"2024-05-27T12:47:58.079331Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"/kaggle/input/bb-rates/BB_Rates_15min_from_10-2023.xlsx\n/kaggle/input/bb-rates/BB_Rates_15min_from_10-2023.xlsx\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-05-27T12:47:58.081816Z","iopub.execute_input":"2024-05-27T12:47:58.082110Z","iopub.status.idle":"2024-05-27T12:47:58.086647Z","shell.execute_reply.started":"2024-05-27T12:47:58.082086Z","shell.execute_reply":"2024-05-27T12:47:58.085524Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"# Meeting 03.05.2024 Christoph-Nils-Arne\nVarianz vorhersagen ist der Kern\n\neinfaches Regressionsmodell \n\nGARCH\nNils LSTM\n\nKaggle aufsetzen \nCopilot\n\n### Packages:\n- AEON \n- SK Time \n- DARts \n\noptimal stopping \n\nLIteratur:\n- https://rdcu.be/dGI38","metadata":{"execution":{"iopub.status.busy":"2024-05-16T09:43:03.773276Z","iopub.execute_input":"2024-05-16T09:43:03.773813Z","iopub.status.idle":"2024-05-16T09:43:03.781133Z","shell.execute_reply.started":"2024-05-16T09:43:03.773781Z","shell.execute_reply":"2024-05-16T09:43:03.779586Z"}}},{"cell_type":"markdown","source":"subset = EURUSD[(EURUSD.index >= pd.to_datetime('2023-10-04')) & (EURUSD.index < pd.to_datetime('2023-10-05'))]\n\n# Anzeigen des extrahierten Subsets# Nils - Arne 16.05.24\n- extract USD - EURO pair from the dataset\n- LinearRegression (ARne)\n- GARCH(Nils)\n\n","metadata":{}},{"cell_type":"code","source":"!pip install arch --target=/kaggle/working/\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot\nfrom arch import arch_model\nfile_path = '/kaggle/input/bb-rates/BB_Rates_15min_from_10-2023.xlsx'\ndata = pd.read_excel(file_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T12:47:58.087850Z","iopub.execute_input":"2024-05-27T12:47:58.088187Z","iopub.status.idle":"2024-05-27T12:48:29.771494Z","shell.execute_reply.started":"2024-05-27T12:47:58.088156Z","shell.execute_reply":"2024-05-27T12:48:29.770538Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Collecting arch\n  Using cached arch-7.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nCollecting numpy>=1.22.3 (from arch)\n  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\nCollecting scipy>=1.8 (from arch)\n  Using cached scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\nCollecting pandas>=1.4 (from arch)\n  Using cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\nCollecting statsmodels>=0.12 (from arch)\n  Using cached statsmodels-0.14.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\nCollecting python-dateutil>=2.8.2 (from pandas>=1.4->arch)\n  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\nCollecting pytz>=2020.1 (from pandas>=1.4->arch)\n  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\nCollecting tzdata>=2022.7 (from pandas>=1.4->arch)\n  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\nCollecting patsy>=0.5.6 (from statsmodels>=0.12->arch)\n  Using cached patsy-0.5.6-py2.py3-none-any.whl.metadata (3.5 kB)\nCollecting packaging>=21.3 (from statsmodels>=0.12->arch)\n  Using cached packaging-24.0-py3-none-any.whl.metadata (3.2 kB)\nCollecting six (from patsy>=0.5.6->statsmodels>=0.12->arch)\n  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\nUsing cached arch-7.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (983 kB)\nUsing cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\nUsing cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\nUsing cached scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\nUsing cached statsmodels-0.14.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\nUsing cached packaging-24.0-py3-none-any.whl (53 kB)\nUsing cached patsy-0.5.6-py2.py3-none-any.whl (233 kB)\nUsing cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\nUsing cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\nUsing cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\nUsing cached six-1.16.0-py2.py3-none-any.whl (11 kB)\nInstalling collected packages: pytz, tzdata, six, packaging, numpy, scipy, python-dateutil, patsy, pandas, statsmodels, arch\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.9.3 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.6 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.2 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.2 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.2 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.2 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.0 which is incompatible.\njupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npylibraft 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\nrmm 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\ntensorstore 0.1.56 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\ntextblob 0.18.0.post0 requires nltk>=3.8, but you have nltk 3.2.4 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.6.4 requires scipy<1.12,>=1.4.1, but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed arch-7.0.0 numpy-1.26.4 packaging-24.0 pandas-2.2.2 patsy-0.5.6 python-dateutil-2.9.0.post0 pytz-2024.1 scipy-1.13.1 six-1.16.0 statsmodels-0.14.2 tzdata-2024.1\n\u001b[33mWARNING: Target directory /kaggle/working/pytz already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/numpy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/patsy-0.5.6.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/statsmodels-0.14.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/packaging-24.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/pandas-2.2.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/dateutil already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/scipy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/patsy already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/doc already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/six.py already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/ci already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/python_dateutil-2.9.0.post0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/scipy-1.13.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/arch-7.0.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/scipy already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/pandas already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/pytz-2024.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/statsmodels already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/packaging already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/tzdata already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/six-1.16.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/arch already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/tzdata-2024.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Target directory /kaggle/working/bin already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"start_index = 3\nEURUSD = data.iloc[start_index:, :2]\n\n# Rename the columns\nEURUSD.columns = ['Dates', 'Close']\n\n# Display the first few rows of the new dataset to verify\nprint(EURUSD.tail())","metadata":{"execution":{"iopub.status.busy":"2024-05-27T12:48:29.773813Z","iopub.execute_input":"2024-05-27T12:48:29.774130Z","iopub.status.idle":"2024-05-27T12:48:29.782371Z","shell.execute_reply.started":"2024-05-27T12:48:29.774101Z","shell.execute_reply":"2024-05-27T12:48:29.781317Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"                     Dates   Close\n13539  2024-04-17 09:30:00  1.0632\n13540  2024-04-17 09:45:00  1.0625\n13541  2024-04-17 10:00:00   1.063\n13542  2024-04-17 10:15:00  1.0642\n13543  2024-04-17 10:30:00  1.0647\n","output_type":"stream"}]},{"cell_type":"code","source":"(13541/4)/24","metadata":{"execution":{"iopub.status.busy":"2024-05-27T12:48:29.786963Z","iopub.execute_input":"2024-05-27T12:48:29.787211Z","iopub.status.idle":"2024-05-27T12:48:29.812782Z"},"trusted":true},"execution_count":null,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"141.05208333333334"},"metadata":{}}]},{"cell_type":"code","source":"print(EURUSD.iloc[0,0])\nprint(EURUSD.iloc[1,0])\nprint(EURUSD.iloc[13540,0] - EURUSD.iloc[0,0])","metadata":{"execution":{"iopub.status.busy":"2024-05-27T12:48:29.813868Z","iopub.execute_input":"2024-05-27T12:48:29.814099Z","iopub.status.idle":"2024-05-27T12:48:29.821201Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"2023-10-03 23:00:00\n2023-10-03 23:15:00\n196 days, 11:30:00\n","output_type":"stream"}]},{"cell_type":"code","source":"85500/60","metadata":{"execution":{"iopub.status.busy":"2024-05-27T12:48:29.822342Z","iopub.execute_input":"2024-05-27T12:48:29.822609Z","iopub.status.idle":"2024-05-27T12:48:29.831131Z"},"trusted":true},"execution_count":null,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"1425.0"},"metadata":{}}]},{"cell_type":"code","source":"# Convert the 'Dates' column to datetime format\nEURUSD['Dates'] = pd.to_datetime(EURUSD['Dates'])\n\n# Calculate the difference between consecutive dates\ntime_diffs = EURUSD['Dates'].diff().dropna()\n\n# Check if all differences are 15 minutes\nis_15min_apart = (time_diffs == pd.Timedelta(minutes=15)).all()\n\nprint(\"Are all dates 15 minutes apart in increasing order?\", is_15min_apart)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T12:48:29.832133Z","iopub.execute_input":"2024-05-27T12:48:29.832440Z","iopub.status.idle":"2024-05-27T12:48:29.849036Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Are all dates 15 minutes apart in increasing order? False\n","output_type":"stream"}]},{"cell_type":"code","source":"#test_EURUSD['Dates'] = EURUSD['Dates'].dt.date()","metadata":{"execution":{"iopub.status.busy":"2024-05-27T12:48:29.851970Z","iopub.execute_input":"2024-05-27T12:48:29.852226Z","iopub.status.idle":"2024-05-27T12:48:29.856170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_diffs_equal_15min = time_diffs == pd.Timedelta(minutes=15)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T12:48:29.857444Z","iopub.execute_input":"2024-05-27T12:48:29.857710Z","iopub.status.idle":"2024-05-27T12:48:29.864363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(time_diffs_equal_15min == False)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T12:48:29.865351Z","iopub.execute_input":"2024-05-27T12:48:29.865608Z","iopub.status.idle":"2024-05-27T12:48:29.878937Z"},"trusted":true},"execution_count":null,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"48"},"metadata":{}}]},{"cell_type":"code","source":"# Find the indices where the time differences are not 15 minutes\nindices_not_15min = time_diffs_equal_15min[~time_diffs_equal_15min].index\n\n# Print the indices\nprint(\"Indices where time differences are not 15 minutes apart:\", indices_not_15min)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T12:48:29.879702Z","iopub.execute_input":"2024-05-27T12:48:29.879936Z","iopub.status.idle":"2024-05-27T12:48:29.887102Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Indices where time differences are not 15 minutes apart: Index([  291,   780,  1272,  1764,  2256,  2747,  2749,  3241,  3243,  3732,\n        3733,  4227,  4228,  4717,  5206,  5692,  5733,  5735,  5737,  5738,\n        5739,  5740,  5741,  6125,  6525,  7016,  7508,  7999,  8493,  8494,\n        8982,  8984,  8985,  9470,  9959, 10449, 10450, 10462, 10846, 11338,\n       11341, 11828, 11829, 12318, 12320, 12808, 13296, 13297],\n      dtype='int64')\n","output_type":"stream"}]},{"cell_type":"code","source":"indices_not_15min[1:] - indices_not_15min[:-1]","metadata":{"execution":{"iopub.status.busy":"2024-05-27T12:48:29.887897Z","iopub.execute_input":"2024-05-27T12:48:29.888118Z","iopub.status.idle":"2024-05-27T12:48:29.898377Z"},"trusted":true},"execution_count":null,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"Index([489, 492, 492, 492, 491,   2, 492,   2, 489,   1, 494,   1, 489, 489,\n       486,  41,   2,   2,   1,   1,   1,   1, 384, 400, 491, 492, 491, 494,\n         1, 488,   2,   1, 485, 489, 490,   1,  12, 384, 492,   3, 487,   1,\n       489,   2, 488, 488,   1],\n      dtype='int64')"},"metadata":{}}]},{"cell_type":"code","source":"(492/4)/24","metadata":{"execution":{"iopub.status.busy":"2024-05-27T12:48:29.899401Z","iopub.execute_input":"2024-05-27T12:48:29.899671Z","iopub.status.idle":"2024-05-27T12:48:29.907498Z"},"trusted":true},"execution_count":null,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"5.125"},"metadata":{}}]},{"cell_type":"code","source":"EURUSD.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EURUSD['Dates'] = pd.to_datetime(EURUSD['Dates'])\n\n# Extrahieren aller Datenpunkte, die zum 4. Oktober 2023 gehören\nsubset = EURUSD[EURUSD['Dates'].dt.date == pd.to_datetime('2023-10-04').date()]","metadata":{"execution":{"iopub.status.busy":"2024-05-27T12:49:31.542647Z","iopub.execute_input":"2024-05-27T12:49:31.543280Z","iopub.status.idle":"2024-05-27T12:49:31.568794Z","shell.execute_reply.started":"2024-05-27T12:49:31.543238Z","shell.execute_reply":"2024-05-27T12:49:31.567795Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"days_full = pd.DataFrame(EURUSD)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T12:52:34.587715Z","iopub.execute_input":"2024-05-27T12:52:34.588323Z","iopub.status.idle":"2024-05-27T12:52:34.592766Z","shell.execute_reply.started":"2024-05-27T12:52:34.588282Z","shell.execute_reply":"2024-05-27T12:52:34.591739Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"days_full['Dates'] = pd.to_datetime(days_full['Dates'])","metadata":{"execution":{"iopub.status.busy":"2024-05-27T12:55:37.269341Z","iopub.execute_input":"2024-05-27T12:55:37.270119Z","iopub.status.idle":"2024-05-27T12:55:37.280125Z","shell.execute_reply.started":"2024-05-27T12:55:37.270089Z","shell.execute_reply":"2024-05-27T12:55:37.279063Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"days_full['Dates'] = days_full['Dates'].dt.date","metadata":{"execution":{"iopub.status.busy":"2024-05-27T12:55:54.474559Z","iopub.execute_input":"2024-05-27T12:55:54.474902Z","iopub.status.idle":"2024-05-27T12:55:54.484573Z","shell.execute_reply.started":"2024-05-27T12:55:54.474873Z","shell.execute_reply":"2024-05-27T12:55:54.483694Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Zählen Sie die Anzahl der Einträge pro Tag\nentries_per_day = days_full.groupby('Dates').size()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T12:57:25.633102Z","iopub.execute_input":"2024-05-27T12:57:25.633869Z","iopub.status.idle":"2024-05-27T12:57:25.642123Z","shell.execute_reply.started":"2024-05-27T12:57:25.633833Z","shell.execute_reply":"2024-05-27T12:57:25.641220Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"\nfull_days = entries_per_day[entries_per_day == 96].index\nprint(full_days)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T12:58:13.460045Z","iopub.execute_input":"2024-05-27T12:58:13.460712Z","iopub.status.idle":"2024-05-27T12:58:13.466495Z","shell.execute_reply.started":"2024-05-27T12:58:13.460679Z","shell.execute_reply":"2024-05-27T12:58:13.465489Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"Index([2023-10-04, 2023-10-05, 2023-10-09, 2023-10-10, 2023-10-11, 2023-10-12,\n       2023-10-16, 2023-10-17, 2023-10-18, 2023-10-19,\n       ...\n       2024-04-01, 2024-04-02, 2024-04-03, 2024-04-04, 2024-04-08, 2024-04-09,\n       2024-04-10, 2024-04-11, 2024-04-15, 2024-04-16],\n      dtype='object', name='Dates', length=109)\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\n# Filtern Sie den ursprünglichen DataFrame, um nur die vollen Tage zu behalten\nfull_days_df = days_full[days_full['Dates'].isin(full_days)]\n\nprint(full_days_df)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T13:12:20.615716Z","iopub.execute_input":"2024-05-27T13:12:20.616689Z","iopub.status.idle":"2024-05-27T13:12:20.628750Z","shell.execute_reply.started":"2024-05-27T13:12:20.616647Z","shell.execute_reply":"2024-05-27T13:12:20.627678Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"            Dates   Close\n7      2023-10-04  1.0468\n8      2023-10-04  1.0467\n9      2023-10-04  1.0467\n10     2023-10-04  1.0465\n11     2023-10-04  1.0465\n...           ...     ...\n13496  2024-04-16  1.0619\n13497  2024-04-16  1.0623\n13498  2024-04-16  1.0618\n13499  2024-04-16  1.0617\n13500  2024-04-16  1.0617\n\n[10464 rows x 2 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"len(full_days_df)\n#full_days_df = full_days_df.set_index('Dates')","metadata":{"execution":{"iopub.status.busy":"2024-05-27T13:12:23.373401Z","iopub.execute_input":"2024-05-27T13:12:23.374353Z","iopub.status.idle":"2024-05-27T13:12:23.379881Z","shell.execute_reply.started":"2024-05-27T13:12:23.374321Z","shell.execute_reply":"2024-05-27T13:12:23.378868Z"},"trusted":true},"execution_count":84,"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"10464"},"metadata":{}}]},{"cell_type":"code","source":"daily_series = []\n\n# Iterieren Sie über die vollen Tage und extrahieren Sie die Zeitreihen\nfor day in full_days:\n    daily_data = full_days_df[full_days_df['Dates'] == day]['Close'].values\n    daily_series.append(daily_data)\n\n# Konvertieren Sie die Liste der Zeitreihen in ein numpy Array\ndaily_series_array = np.array(daily_series, dtype=np.float32)\n\nprint(daily_series_array)\nfull_days_df = full_days_df.set_index('Dates')","metadata":{"execution":{"iopub.status.busy":"2024-05-27T13:12:24.854501Z","iopub.execute_input":"2024-05-27T13:12:24.855243Z","iopub.status.idle":"2024-05-27T13:12:25.102620Z","shell.execute_reply.started":"2024-05-27T13:12:24.855211Z","shell.execute_reply":"2024-05-27T13:12:25.101516Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"[[1.0468 1.0467 1.0467 ... 1.0506 1.0506 1.0507]\n [1.0508 1.0508 1.0506 ... 1.0549 1.0549 1.0549]\n [1.0552 1.0552 1.0555 ... 1.0566 1.0566 1.0566]\n ...\n [1.0741 1.0743 1.0743 ... 1.0727 1.0728 1.0727]\n [1.0642 1.0648 1.0651 ... 1.0625 1.0625 1.0625]\n [1.0626 1.0627 1.0627 ... 1.0618 1.0617 1.0617]]\n","output_type":"stream"}]},{"cell_type":"code","source":"train_size = 0.6\nval_size = 0.2\ntest_size = 0.2\n\n# Berechnen Sie die Anzahl der Tage für jeden Satz\nnum_days = daily_series_array.shape[0]\ntrain_end = int(num_days * train_size)\nval_end = train_end + int(num_days * val_size)\n\n# Aufteilen der Daten in Trainings-, Validierungs- und Testsätze\ntrain_data = daily_series_array[:train_end]\nval_data = daily_series_array[train_end:val_end]\ntest_data = daily_series_array[val_end:]\n\nprint(\"Trainingsdaten:\", train_data.shape)\nprint(\"Validierungsdaten:\", val_data.shape)\nprint(\"Testdaten:\", test_data.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T13:12:27.982546Z","iopub.execute_input":"2024-05-27T13:12:27.983353Z","iopub.status.idle":"2024-05-27T13:12:27.989678Z","shell.execute_reply.started":"2024-05-27T13:12:27.983322Z","shell.execute_reply":"2024-05-27T13:12:27.988817Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"Trainingsdaten: (65, 96)\nValidierungsdaten: (21, 96)\nTestdaten: (23, 96)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        return torch.tensor(sample, dtype=torch.float32)\n\n# Erstellen Sie Dataset-Objekte für Trainings-, Validierungs- und Testsätze\ntrain_dataset = TimeSeriesDataset(train_data)\nval_dataset = TimeSeriesDataset(val_data)\ntest_dataset = TimeSeriesDataset(test_data)\n\n# Erstellen Sie DataLoader-Objekte\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Beispiel: Durchlaufen der Daten mit dem DataLoader\nfor batch in train_loader:\n    print(batch)\n    break\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T13:12:30.079863Z","iopub.execute_input":"2024-05-27T13:12:30.080503Z","iopub.status.idle":"2024-05-27T13:12:30.140657Z","shell.execute_reply.started":"2024-05-27T13:12:30.080472Z","shell.execute_reply":"2024-05-27T13:12:30.139816Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"tensor([[1.0538, 1.0539, 1.0538,  ..., 1.0582, 1.0582, 1.0582],\n        [1.0881, 1.0882, 1.0881,  ..., 1.0849, 1.0848, 1.0850],\n        [1.0798, 1.0799, 1.0799,  ..., 1.0874, 1.0878, 1.0878],\n        ...,\n        [1.0916, 1.0916, 1.0919,  ..., 1.0885, 1.0885, 1.0886],\n        [1.0524, 1.0525, 1.0521,  ..., 1.0560, 1.0560, 1.0560],\n        [1.0886, 1.0884, 1.0882,  ..., 1.0846, 1.0847, 1.0847]])\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn as nn\n\nclass EncoderLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers):\n        super(EncoderLSTM, self).__init__()\n        \n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        \n    def forward(self, x):\n        # x: (batch_size, seq_len, input_size)\n        _, (h_n, c_n) = self.lstm(x)\n        # h_n: (num_layers, batch_size, hidden_size)\n        # c_n: (num_layers, batch_size, hidden_size)\n        return h_n, c_n\n        \nclass DecoderLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size):\n        super(DecoderLSTM, self).__init__()\n        \n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n        \n    def forward(self, x, h_n, c_n):\n        # x: (batch_size, seq_len, input_size)\n        # h_n: (num_layers, batch_size, hidden_size)\n        # c_n: (num_layers, batch_size, hidden_size)\n        output, _ = self.lstm(x, (h_n, c_n))\n        # output: (batch_size, seq_len, hidden_size)\n        output = self.fc(output)\n        # output: (batch_size, seq_len, output_size)\n        return output\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T13:16:44.254303Z","iopub.execute_input":"2024-05-27T13:16:44.254684Z","iopub.status.idle":"2024-05-27T13:16:44.263607Z","shell.execute_reply.started":"2024-05-27T13:16:44.254657Z","shell.execute_reply":"2024-05-27T13:16:44.262594Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"class VAE_LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, latent_space_dim, num_layers):\n        super(VAE_LSTM, self).__init__()\n        \n        self.encoder = EncoderLSTM(input_size, hidden_size, num_layers)\n        \n        self.mu_layer = nn.Linear(hidden_size, latent_space_dim)\n        self.logvar_layer = nn.Linear(hidden_size, latent_space_dim)\n        \n        self.z_layer = nn.Linear(latent_space_dim, hidden_size)\n        \n        self.decoder = DecoderLSTM(latent_space_dim, hidden_size, num_layers, input_size)\n            \n    def forward(self, x):\n        h_n, c_n = self.encoder(x)\n        \n        mu = self.mu_layer(h_n[-1])  # Verwende den letzten hidden state als Eingabe für mu und logvar\n        logvar = self.logvar_layer(h_n[-1])\n        \n        if self.training:\n            sigma = torch.exp(0.5 * logvar)\n            epsilon = torch.randn_like(sigma)\n            z = mu + epsilon * sigma\n        else:\n            z = mu\n        \n        z = self.z_layer(z)\n        z = z.unsqueeze(1).repeat(1, x.size(1), 1)  # Wiederhole z für jede Zeitschritt\n        output = self.decoder(z, h_n, c_n)\n        \n        return output, mu, logvar\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T13:17:02.993383Z","iopub.execute_input":"2024-05-27T13:17:02.994049Z","iopub.status.idle":"2024-05-27T13:17:03.002915Z","shell.execute_reply.started":"2024-05-27T13:17:02.994017Z","shell.execute_reply":"2024-05-27T13:17:03.001779Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"def vae_loss(recon_x, x, mu, logvar):\n    # Reconstruction losses are calculated using Mean Squared Error (MSE) and \n    # summed over all elements and batch\n    mse_loss = nn.MSELoss(reduction = 'sum')(recon_x, x)\n\n    # See Appendix B from VAE paper:\n    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n    # https://arxiv.org/abs/1312.6114\n    kld_loss = 1/2*torch.sum(1+logvar -mu.pow(2) - logvar.exp())\n\n    total_loss = mse_loss - kld_loss\n\n    return total_loss, mse_loss, kld_loss","metadata":{"execution":{"iopub.status.busy":"2024-05-27T13:17:29.832587Z","iopub.execute_input":"2024-05-27T13:17:29.833453Z","iopub.status.idle":"2024-05-27T13:17:29.838819Z","shell.execute_reply.started":"2024-05-27T13:17:29.833418Z","shell.execute_reply":"2024-05-27T13:17:29.837885Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"learning_rate= 0.001\ninput_size = 1\nhidden_size = 64\nlatent_space_dim = 16\nnum_layers = 2\nepochs = 25","metadata":{"execution":{"iopub.status.busy":"2024-05-27T13:17:40.243460Z","iopub.execute_input":"2024-05-27T13:17:40.244155Z","iopub.status.idle":"2024-05-27T13:17:40.248809Z","shell.execute_reply.started":"2024-05-27T13:17:40.244122Z","shell.execute_reply":"2024-05-27T13:17:40.247846Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"def get_device(cuda_preference=True):\n    \"\"\"Gets pytorch device object. If cuda_preference=True and \n        cuda is available on your system, returns a cuda device.\n    \n    Args:\n        cuda_preference: bool, default True\n            Set to true if you would like to get a cuda device\n            \n    Returns: pytorch device object\n            Pytorch device\n    \"\"\"\n    \n    print('cuda available:', torch.cuda.is_available(), \n          '; cudnn available:', torch.backends.cudnn.is_available(),\n          '; num devices:', torch.cuda.device_count())\n    \n    use_cuda = False if not cuda_preference else torch.cuda.is_available()\n    device = torch.device('cuda:0' if use_cuda else 'cpu')\n    device_name = torch.cuda.get_device_name(device) if use_cuda else 'cpu'\n    print('Using device', device_name)\n    return device","metadata":{"execution":{"iopub.status.busy":"2024-05-27T13:18:37.568713Z","iopub.execute_input":"2024-05-27T13:18:37.569064Z","iopub.status.idle":"2024-05-27T13:18:37.575590Z","shell.execute_reply.started":"2024-05-27T13:18:37.569036Z","shell.execute_reply":"2024-05-27T13:18:37.574580Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"! python --version\n\ntorch.cuda.is_available(), torch.backends.cudnn.is_available(), torch.cuda.device_count()\n\ndevice = get_device()\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\nnum_cpus = os.cpu_count()\n\nprint(f\"cpu_count: {num_cpus}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-27T13:18:48.814512Z","iopub.execute_input":"2024-05-27T13:18:48.814838Z","iopub.status.idle":"2024-05-27T13:18:49.845922Z","shell.execute_reply.started":"2024-05-27T13:18:48.814814Z","shell.execute_reply":"2024-05-27T13:18:49.844730Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stdout","text":"Python 3.10.13\ncuda available: True ; cudnn available: True ; num devices: 2\nUsing device Tesla T4\nUsing device: cuda\ncpu_count: 4\n","output_type":"stream"}]},{"cell_type":"code","source":"model = VAE_LSTM(input_size,hidden_size, latent_space_dim, num_layers)\nmodel = model.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T13:31:16.872589Z","iopub.execute_input":"2024-05-27T13:31:16.873347Z","iopub.status.idle":"2024-05-27T13:31:16.883965Z","shell.execute_reply.started":"2024-05-27T13:31:16.873314Z","shell.execute_reply":"2024-05-27T13:31:16.883100Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = model.to(device)\ntotal_losses, mse_losses, kl_losses, test_total_losses, test_mse_losses, test_kl_losses = run_training(\n        model, torch.optim.Adam(model.parameters(), lr=learning_rate), vae_loss, epochs, train_loader, test_loader)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T13:35:37.303584Z","iopub.execute_input":"2024-05-27T13:35:37.304475Z","iopub.status.idle":"2024-05-27T13:35:37.612024Z","shell.execute_reply.started":"2024-05-27T13:35:37.304432Z","shell.execute_reply":"2024-05-27T13:35:37.610592Z"},"trusted":true},"execution_count":104,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      <progress value='0' class='' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      0.00% [0/25 00:00&lt;?]\n    </div>\n    \n\n\n    <div>\n      <progress value='0' class='' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      0.00% [0/3 00:00&lt;?]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:879: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/cudnn/RNN.cpp:982.)\n  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[104], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m total_losses, mse_losses, kl_losses, test_total_losses, test_mse_losses, test_kl_losses \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvae_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[103], line 64\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(model, optimizer, loss_function, num_epochs, train_dataloader, test_dataloader, verbose)\u001b[0m\n\u001b[1;32m     60\u001b[0m test_total_losses, test_mse_losses, test_kl_losses \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m master_bar:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     total_loss, mse_loss, kl_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaster_bar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     test_total_loss, test_mse_loss, test_kl_loss \u001b[38;5;241m=\u001b[39m test_set_loss(test_dataloader, model, \n\u001b[1;32m     69\u001b[0m                                                                  loss_function, master_bar)\n","Cell \u001b[0;32mIn[103], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, optimizer, model, loss_fn, master_bar)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# print(type(inputs[0]))\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# inputs = [tensor.to(device) for tensor in inputs]\u001b[39;00m\n\u001b[1;32m     17\u001b[0m  \u001b[38;5;66;03m#print(inputs.size())\u001b[39;00m\n\u001b[1;32m     18\u001b[0m  optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 20\u001b[0m  pred, mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m  t_loss, m_loss, k_loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, inputs, mu, logvar)\n\u001b[1;32m     23\u001b[0m  t_loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[89], line 15\u001b[0m, in \u001b[0;36mVAE_LSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 15\u001b[0m     h_n, c_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     mu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmu_layer(h_n[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Verwende den letzten hidden state als Eingabe für mu und logvar\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogvar_layer(h_n[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[88], line 11\u001b[0m, in \u001b[0;36mEncoderLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# x: (batch_size, seq_len, input_size)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     _, (h_n, c_n) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# h_n: (num_layers, batch_size, hidden_size)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# c_n: (num_layers, batch_size, hidden_size)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m h_n, c_n\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:879\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    876\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    882\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    883\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n","\u001b[0;31mRuntimeError\u001b[0m: shape '[24576, 1]' is invalid for input of size 256"],"ename":"RuntimeError","evalue":"shape '[24576, 1]' is invalid for input of size 256","output_type":"error"}]}]}